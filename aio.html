<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Fundamentals of Classification in Machine Learning: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="assets/images/incubator-logo.svg">
</div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Fundamentals of Classification in Machine Learning
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Classification in Machine Learning
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Classification in Machine Learning
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction to Deep Learning</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-image-data.html">2. Introduction to Image Data</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-build-cnn.html">3. Build a Convolutional Neural Network</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction to Deep Learning</a></p>
<hr>
<p>Last updated on 2025-06-11 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is machine learning and what is it used for?</li>
<li>What is deep learning?</li>
<li>How do I use a neural network for image classification?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the difference between artificial intelligence, machine
learning and deep learning.</li>
<li>Understand the different types of computer vision tasks.</li>
<li>Perform an image classification using a convolutional neural network
(CNN).</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="deep-learning-machine-learning-and-artificial-intelligence">Deep Learning, Machine Learning and Artificial Intelligence<a class="anchor" aria-label="anchor" href="#deep-learning-machine-learning-and-artificial-intelligence"></a>
</h2>
<hr class="half-width">
<p>Artificial Intelligence (AI) is the broad field that involves
creating machines capable of performing tasks that typically require
human intelligence. This includes everything from recognizing speech and
images to making decisions and translating languages. Within AI, Machine
Learning (ML) is a subset focused on the development of algorithms that
allow computers to learn and improve from experience without being
explicitly programmed.</p>
<p>Deep Learning (DL), a further subset of ML, utilizes neural networks
with many layers (hence “deep”) to model complex patterns in large
amounts of data. This technique has led to significant advances in
various fields, such as image and speech recognition.</p>
<p>Since the 1950s, the idea of AI has captured the imagination of many,
often depicted in science fiction as machines with human-like or even
superior intelligence. While recent advancements in AI and ML have been
remarkable, we are currently capable of achieving human-like
intelligence only in specific areas. The goal of creating a
general-purpose AI, one that can perform any intellectual task a human
can, remains a long-term challenge.</p>
<p>The image below illustrates some differences between artificial
intelligence, machine learning and deep learning.</p>
<figure><img src="fig/01_AI_ML_DL_differences.png" alt="Three nested circles defining deep learning as a subset of machine learning which is a subset of artifical intelligence" class="figure mx-auto d-block"><div class="figcaption">The image above is by Tukijaaliwa, CC BY-SA 4.0,
via Wikimedia Commons, <a href="https://en.wikipedia.org/wiki/File:AI-ML-DL.svg" class="external-link">original
source</a>
</div>
</figure><!-- Collect your link references at the bottom of your document --><!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 --></section></section><section id="aio-02-image-data"><p>Content from <a href="02-image-data.html">Introduction to Image Data</a></p>
<hr>
<p>Last updated on 2025-06-11 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/02-image-data.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How much data do you need for Deep Learning?</li>
<li>How do I prepare image data for use in a convolutional neural
network (CNN)?</li>
<li>How do I work with image data in python?</li>
<li>Where can I find image data to train my model?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the properties of image data.</li>
<li>Write code to prepare an image dataset to train a CNN.</li>
<li>Know the difference between training, testing, and validation
datasets.</li>
<li>Identify sources of image data.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="deep-learning-workflow">Deep Learning Workflow<a class="anchor" aria-label="anchor" href="#deep-learning-workflow"></a>
</h2>
<hr class="half-width">
<p>Let’s start over from the beginning of our workflow.</p>
<div class="section level3">
<h3 id="step-1--formulate-outline-the-problem">Step 1. Formulate/ Outline the problem<a class="anchor" aria-label="anchor" href="#step-1--formulate-outline-the-problem"></a>
</h3>
<p>Firstly we must decide what it is we want our Deep Learning system to
do. This lesson is all about image classification and our aim is to put
an image into one of ten categories: airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, or truck</p>
</div>
<div class="section level3">
<h3 id="step-2--identify-inputs-and-outputs">Step 2. Identify inputs and outputs<a class="anchor" aria-label="anchor" href="#step-2--identify-inputs-and-outputs"></a>
</h3>
<p>Next we identify the inputs and outputs of the neural network. In our
case, the data is images and the inputs could be the individual pixels
of the images.</p>
<p>We are performing a classification problem and we want to output one
category for each image.</p>
</div>
<div class="section level3">
<h3 id="step-3--prepare-data">Step 3. Prepare data<a class="anchor" aria-label="anchor" href="#step-3--prepare-data"></a>
</h3>
<p>Deep Learning requires extensive training data which tells the
network what output it should produce for a given input. In this
workshop, our network will be trained on a series of images and told
what they contain. Once the network is trained, it should be able to
take another image and correctly classify its contents.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>How much data do you need for Deep Learning?</p>
<p>Unfortunately, this question is not easy to answer. It depends, among
other things, on the complexity of the task (which you often do not know
beforehand), the quality of the available dataset and the complexity of
the network. For complex tasks with large neural networks, adding more
data often improves performance. However, this is also not a generic
truth: if the data you add is too similar to the data you already have,
it will not give much new information to the neural network.</p>
<p>In case you have too little data available to train a complex network
from scratch, it is sometimes possible to use a pretrained network that
was trained on a similar problem. Another trick is data augmentation,
where you expand the dataset with artificial data points that could be
real. An example of this is mirroring images when trying to classify
cats and dogs. An horizontally mirrored animal retains the label, but
exposes a different view.</p>
</div>
</div>
</div>
<p>Depending on your situation, you will prepare your own custom data
for training or use pre-existing data.</p>
</div>
</section><section><h2 class="section-heading" id="custom-image-data">Custom image data<a class="anchor" aria-label="anchor" href="#custom-image-data"></a>
</h2>
<hr class="half-width">
<p>In some cases, you will create your own set of labelled images.</p>
<p>The steps to prepare your own custom image data include:</p>
<p><strong>Custom data i. Data collection and Labeling:</strong></p>
<p>For image classification the label applies to the entire image;
object detection requires bounding boxes around objects of interest, and
instance or semantic segmentation requires each pixel to be
labelled.</p>
<p>There are a number of open source software used to label your
dataset, including:</p>
<ul>
<li>(Visual Geometry Group) <a href="https://www.robots.ox.ac.uk/~vgg/software/via/" class="external-link">VGG Image
Annotator</a> (VIA)</li>
<li>
<a href="https://imagej.net/" class="external-link">ImageJ</a> can be extended with
plugins for annotation</li>
<li>
<a href="https://github.com/jsbroks/coco-annotator" class="external-link">COCO
Annotator</a> is designed specifically for creating annotations
compatible with Common Objects in Context (COCO) format</li>
</ul>
<p><strong>Custom data ii. Data preprocessing:</strong></p>
<p>This step involves various tasks to enhance the quality and
consistency of the data:</p>
<ul>
<li><p><strong>Resizing</strong>: Resize images to a consistent
resolution to ensure uniformity and reduce computational load.</p></li>
<li><p><strong>Augmentation</strong>: Apply random transformations
(e.g., rotations, flips, shifts) to create new variations of the same
image. This helps improve the model’s robustness and generalisation by
exposing it to more diverse data.</p></li>
<li><p><strong>Normalisation</strong>: Scale pixel values to a common
range, often between 0 and 1 or -1 and 1. Normalisation helps the model
converge faster during training.</p></li>
<li><p><strong>Label encoding</strong> is a technique used to represent
categorical data with numerical labels.</p></li>
<li><p><strong>Data Splitting</strong>: Split the data set into separate
parts to have one for training, one for evaluating the model’s
performance during training, and one reserved for the final evaluation
of the model’s performance.</p></li>
</ul>
<p>Before jumping into these specific preprocessing tasks, it’s
important to understand that images on a computer are stored as
numerical representations or simplified versions of the real world.
Therefore it’s essential to take some time to understand these numerical
abstractions.</p>
<div class="section level3">
<h3 id="pixels">Pixels<a class="anchor" aria-label="anchor" href="#pixels"></a>
</h3>
<p>Images on a computer are stored as rectangular arrays of hundreds,
thousands, or millions of discrete “picture elements,” otherwise known
as pixels. Each pixel can be thought of as a single square point of
coloured light.</p>
<p>For example, consider this image of a Jabiru, with a square area
designated by a red box:</p>
<figure><img src="fig/02_Jabiru_TGS_marked.jpg" alt="Jabiru image that is 552 pixels wide and 573 pixels high. A red square around the neck region indicates the area to zoom in on." class="figure mx-auto d-block"></figure><p>Now, if we zoomed in close enough to the red box, the individual
pixels would stand out:</p>
<figure><img src="fig/02_Jabiru_TGS_marked_zoom_enlarged.jpg" alt="zoomed in area of Jabiru where the individual pixels stand out" class="figure mx-auto d-block"></figure><p>Note each square in the enlarged image area (i.e. each pixel) is all
one colour, but each pixel can be a different colour from its
neighbours. Viewed from a distance, these pixels seem to blend together
to form the image.</p>
</div>
<div class="section level3">
<h3 id="working-with-pixels">Working with Pixels<a class="anchor" aria-label="anchor" href="#working-with-pixels"></a>
</h3>
<p>In python, an image can represented as a 2- or 3-dimensional array.
An <strong>array</strong> is used to store multiple values or elements
of the same datatype in a single variable. In the context of images,
arrays have dimensions for height, width, and colour channels (if
applicable) and each element corresponds to a pixel value in the image.
Let us start with the Jabiru image.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># specify the image path</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>new_img_path <span class="op">=</span> <span class="st">"../data/Jabiru_TGS.JPG"</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># read in the image with default arguments</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>new_img_pil <span class="op">=</span> keras.utils.load_img(path<span class="op">=</span>new_img_path)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># check the image class and size</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image class :'</span>, new_img_pil.__class__)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Image size:'</span>, new_img_pil.size)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Image class : &lt;class 'PIL.JpegImagePlugin.JpegImageFile'&gt;
Image size (552, 573)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="image-dimensions---resizing">Image Dimensions - Resizing<a class="anchor" aria-label="anchor" href="#image-dimensions---resizing"></a>
</h3>
<p>The new image has shape <code>(573, 552)</code>, meaning it is much
larger in size, 573x552 pixels and is a rectangle instead of a
square.</p>
<p>Recall from the introduction that our training data set consists of
50000 images of 32x32 pixels.</p>
<p>To reduce the computational load and ensure all of our images have a
uniform size, we need to choose an image resolution (or size in pixels)
and ensure all of the images we use are resized to be consistent.</p>
<p>There are a couple of ways to do this in python but one way is to
specify the size you want using the <code>target_size</code> argument to
the <code>keras.utils.load_img()</code> function.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># read in the image and specify the target size</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>new_img_pil_small <span class="op">=</span> keras.utils.load_img(path<span class="op">=</span>new_img_path, target_size<span class="op">=</span>(<span class="dv">32</span>,<span class="dv">32</span>))</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># confirm the image class and size</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Resized image class :'</span>, new_img_pil_small.__class__)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Resized image size'</span>, new_img_pil_small.size) </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Resized image class : &lt;class 'PIL.Image.Image'&gt;
Resized image size (32, 32)</code></pre>
</div>
<p>Of course, if there are a large number of images to preprocess you do
not want to copy and paste these steps for each image! Fortunately,
Keras has a solution: <a href="https://keras.io/api/data_loading/image/" class="external-link">keras.utils.image_dataset_from_directory()</a></p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div> WANT TO KNOW MORE: Python image libraries </h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p>Two of the most commonly used libraries for image representation and
manipulation are NumPy and Pillow (PIL). Additionally, when working with
deep learning frameworks like TensorFlow and PyTorch, images are often
represented as tensors within these frameworks.</p>
<ul>
<li>NumPy is a powerful library for numerical computing in Python. It
provides support for creating and manipulating arrays, which can be used
to represent images as multidimensional arrays.
<ul>
<li><code>import numpy as np</code></li>
</ul>
</li>
<li>The Pillow library provides functions to open, manipulate, and save
various image file formats. It represents images using its own Image
class.
<ul>
<li><code>from PIL import Image</code></li>
<li>
<a href="https://pillow.readthedocs.io/en/latest/reference/Image.html" class="external-link">PIL
Image Module</a> documentation</li>
</ul>
</li>
<li>TensorFlow images are often represented as tensors that have
dimensions for batch size, height, width, and colour channels. This
framework provide tools to load, preprocess, and work with image data
seamlessly.
<ul>
<li><code>from tensorflow import keras</code></li>
<li>
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image" class="external-link">image
preprocessing</a> documentation</li>
<li>Note Keras image functions also use PIL</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-augmentation">Image augmentation<a class="anchor" aria-label="anchor" href="#image-augmentation"></a>
</h3>
<p>There are several ways to augment your data to increase the diversity
of the training data and improve model robustness.</p>
<ul>
<li>Geometric Transformations
<ul>
<li>rotation, scaling, zooming, cropping</li>
</ul>
</li>
<li>Flipping or Mirroring
<ul>
<li>some classes, like horse, have a different shape when facing left or
right and you want your model to recognize both</li>
</ul>
</li>
<li>Colour properties
<ul>
<li>brightness, contrast, or hue</li>
<li>these changes simulate variations in lighting conditions</li>
</ul>
</li>
</ul>
<p>We will not perform image augmentation in this lesson, but it is
important that you are aware of this type of data preparation because it
can make a big difference in your model’s ability to predict outside of
your training data.</p>
<p>Information about these operations are included in the Keras document
for <a href="https://keras.io/api/layers/preprocessing_layers/image_augmentation/" class="external-link">Image
augmentation layers</a>.</p>
</div>
<div class="section level3">
<h3 id="normalisation">Normalisation<a class="anchor" aria-label="anchor" href="#normalisation"></a>
</h3>
<p>Image RGB values are between 0 and 255. As input for neural networks,
it is better to have small input values. The process of converting the
RGB values to be between 0 and 1 is called
<strong>normalisation</strong>.</p>
<p>Before we can normalise our image values we must convert the image to
an numpy array.</p>
<p>We introduced how to do this in <a href="01-introduction.html">Episode 01 Introduction to Deep Learning</a>
but what you may not have noticed is that the
<code>keras.datasets.cifar10.load_data()</code> function did the
conversion for us whereas now we will do it ourselves.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># first convert the image into an array for normalisation</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>new_img_arr <span class="op">=</span> keras.utils.img_to_array(new_img_pil_small)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># confirm the image class and shape</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Converted image class  :'</span>, new_img_arr.__class__)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Converted image shape'</span>, new_img_arr.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Converted image class  : &lt;class 'numpy.ndarray'&gt;
Converted image shape (32, 32, 3)</code></pre>
</div>
<p>Now that the image is an array, we can normalise the values. Let us
also investigate the image values before and after we normalise
them.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># inspect pixel values before and after normalisation</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values BEFORE</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'BEFORE normalisation'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Min pixel value '</span>, new_img_arr.<span class="bu">min</span>()) </span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Max pixel value '</span>, new_img_arr.<span class="bu">max</span>())</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean pixel value '</span>, new_img_arr.mean().<span class="bu">round</span>())</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="co"># normalise the RGB values to be between 0 and 1</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>new_img_arr_norm <span class="op">=</span> new_img_arr <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="co"># extract the min, max, and mean pixel values AFTER</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'AFTER normalisation'</span>) </span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Min pixel value '</span>, new_img_arr_norm.<span class="bu">min</span>()) </span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Max pixel value '</span>, new_img_arr_norm.<span class="bu">max</span>())</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean pixel value '</span>, new_img_arr_norm.mean().<span class="bu">round</span>())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>BEFORE normalisation
Min pixel value  0.0
Max pixel value  255.0
Mean pixel value  87.0
AFTER normalisation
Min pixel value  0.0
Max pixel value  1.0
Mean pixel value  0.0</code></pre>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div> WANT TO KNOW MORE: Why normalise? </h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p>ChatGPT</p>
<p>Normalizing the RGB values to be between 0 and 1 is a common
pre-processing step in machine learning tasks, especially when dealing
with image data. This normalisation has several benefits:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Numerical Stability</strong>: By scaling the RGB values
to a range between 0 and 1, you avoid potential numerical instability
issues that can arise when working with large values. Neural networks
and many other machine learning algorithms are sensitive to the scale of
input features, and normalizing helps to keep the values within a
manageable range.</p></li>
<li><p><strong>Faster Convergence</strong>: Normalizing the RGB values
often helps in faster convergence during the training process. Neural
networks and other optimisation algorithms rely on gradient descent
techniques, and having inputs in a consistent range aids in smoother and
faster convergence.</p></li>
<li><p><strong>Equal Weightage for All Channels</strong>: In RGB images,
each channel (Red, Green, Blue) represents different colour intensities.
By normalizing to the range [0, 1], you ensure that each channel is
treated with equal weightage during training. This is important because
some machine learning algorithms could assign more importance to larger
values.</p></li>
<li><p><strong>Generalisation</strong>: Normalisation helps the model to
generalize better to unseen data. When the input features are in the
same range, the learned weights and biases can be more effectively
applied to new examples, making the model more robust.</p></li>
<li><p><strong>Compatibility</strong>: Many image-related libraries,
algorithms, and models expect pixel values to be in the range of [0, 1].
By normalizing the RGB values, you ensure compatibility and seamless
integration with these tools.</p></li>
</ol>
<p>The normalisation process is typically done by dividing each RGB
value (ranging from 0 to 255) by 255, which scales the values to the
range [0, 1].</p>
<p>For example, if you have an RGB image with pixel values (100, 150,
200), after normalisation, the pixel values would become (100/255,
150/255, 200/255) ≈ (0.39, 0.59, 0.78).</p>
<p>Remember that normalisation is not always mandatory, and there could
be cases where other scaling techniques might be more suitable based on
the specific problem and data distribution. However, for most
image-related tasks in machine learning, normalizing RGB values to [0,
1] is a good starting point.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="one-hot-encoding">One-hot encoding<a class="anchor" aria-label="anchor" href="#one-hot-encoding"></a>
</h3>
<p>A neural network can only take numerical inputs and outputs, and
learns by calculating how “far away” the class predicted by the neural
network is from the true class. When the target (label) is categorical
data, or strings, it is very difficult to determine this “distance” or
error. Therefore we will transform this column into a more suitable
format. There are many ways to do this, however we will be using
<strong>one-hot encoding</strong>.</p>
<p>One-hot encoding is a technique to represent categorical data as
binary vectors, making it compatible with machine learning algorithms.
Each category becomes a separate column, and the presence or absence of
a category is indicated by 1s and 0s in the respective columns.</p>
<p>Let’s say you have a dataset with a “colour” column containing three
categories: yellow, orange, purple.</p>
<p>Table 1. Original Data.</p>
<table class="table">
<thead><tr class="header">
<th>colour</th>
<th align="right"></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>yellow</td>
<td align="right"><span class="emoji" data-emoji="yellow_square">🟨</span></td>
</tr>
<tr class="even">
<td>orange</td>
<td align="right"><span class="emoji" data-emoji="orange_square">🟧</span></td>
</tr>
<tr class="odd">
<td>purple</td>
<td align="right"><span class="emoji" data-emoji="purple_square">🟪</span></td>
</tr>
<tr class="even">
<td>yellow</td>
<td align="right"><span class="emoji" data-emoji="yellow_square">🟨</span></td>
</tr>
</tbody>
</table>
<p>Table 2. After One-Hot Encoding.</p>
<table class="table">
<thead><tr class="header">
<th>colour_yellow</th>
<th align="center">colour_orange</th>
<th align="right">colour_purple</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>0</td>
<td align="center">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>0</td>
<td align="center">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td>1</td>
<td align="center">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>The Keras function for one_hot encoding is called <a href="https://keras.io/api/utils/python_utils/#to_categorical-function" class="external-link">to_categorical</a>:</p>
<p><code>keras.utils.to_categorical(y, num_classes=None, dtype="float32")</code></p>
<ul>
<li>
<code>y</code> is an array of class values to be converted into a
matrix (integers from 0 to num_classes - 1).</li>
<li>
<code>num_classes</code> is the total number of classes. If None,
this would be inferred as max(y) + 1.</li>
<li>
<code>dtype</code> is the data type expected by the input. Default:
‘float32’</li>
</ul>
</div>
<div class="section level3">
<h3 id="data-splitting">Data Splitting<a class="anchor" aria-label="anchor" href="#data-splitting"></a>
</h3>
<p>The typical practice in machine learning is to split your data into
two subsets: a <strong>training</strong> set and a <strong>test</strong>
set. This initial split separates the data you will use to train your
model (Step 6) from the data you will use to evaluate its performance
(Step 8).</p>
<p>After this initial split, you can choose to further split the
training set into a training set and a <strong>validation set</strong>.
This is often done when you are fine-tuning hyperparameters, selecting
the best model from a set of candidate models, or preventing
overfitting.</p>
<p>To split a dataset into training and test sets there is a very
convenient function from sklearn called <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" class="external-link">train_test_split</a>:</p>
<p><code>sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)</code></p>
<ul>
<li>The first two parameters are the dataset (X) and the corresponding
targets (y) (i.e. class labels).</li>
<li>
<code>test_size</code> is the fraction of the dataset used for
testing</li>
<li>
<code>random_state</code> controls the shuffling of the dataset,
setting this value will reproduce the same results (assuming you give
the same integer) every time it is called.</li>
<li>
<code>shuffle</code> controls whether the order of the rows of the
dataset is shuffled before splitting and can be either <code>True</code>
or <code>False</code>.</li>
<li>
<code>stratify</code> is a more advanced parameter that controls how
the split is done.</li>
</ul>
</div>
</section><section><h2 class="section-heading" id="pre-existing-image-data">Pre-existing image data<a class="anchor" aria-label="anchor" href="#pre-existing-image-data"></a>
</h2>
<hr class="half-width">
<p>In other cases you will be able to download an image dataset that is
already labelled and can be used to classify a number of different
object like the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="external-link">CIFAR-10</a> dataset.
Other examples include:</p>
<ul>
<li>
<a href="https://en.wikipedia.org/wiki/MNIST_database" class="external-link">MNIST
database</a> - 60,000 training images of handwritten digits (0-9)</li>
<li>
<a href="https://www.image-net.org/" class="external-link">ImageNet</a> - 14 million
hand-annotated images indicating objects from more than 20,000
categories. ImageNet sponsors an <a href="https://www.image-net.org/challenges/LSVRC/#:~:text=The%20ImageNet%20Large%20Scale%20Visual,image%20classification%20at%20large%20scale." class="external-link">annual
software contest</a> where programs compete to achieve the highest
accuracy. When choosing a pretrained network, the winners of these sorts
of competitions are generally a good place to start.</li>
<li>
<a href="https://cocodataset.org/#home" class="external-link">MS COCO</a> - &gt;200,000
labelled images used for object detection, instance segmentation,
keypoint analysis, and captioning</li>
</ul>
<p>Where labelled data exists, in most cases the data provider or other
users will have created data-specific functions you can use to load the
data. We already did this in the introduction:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co"># load the CIFAR-10 dataset included with the keras library</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>(train_images, train_labels), (test_images, test_labels) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="co"># create a list of classnames associated with each CIFAR-10 label</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>class_names <span class="op">=</span> [<span class="st">'airplane'</span>, <span class="st">'automobile'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>, <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>]</span></code></pre>
</div>
<p>In this instance the data is likely already prepared for use in a
CNN. However, it is always a good idea to first read any associated
documentation to find out what steps the data providers took to prepare
the images and second to take a closer at the images once loaded and
query their attributes.</p>
<p>In our case, we still want prepare the dataset with these steps:</p>
<ul>
<li>normalise the image pixel values to be between 0 and 1</li>
<li>one-hot encode the training image labels</li>
<li>divide the <strong>training data</strong> into
<strong>training</strong> and <strong>validation</strong> sets</li>
</ul>
<p>We performed these operations in <strong>Step 3. Prepare
data</strong> of the Introduction but let us create the function to
prepare the dataset again knowing what we know now.</p>
<div id="challenge-create-a-function-to-prepare-the-dataset" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-function-to-prepare-the-dataset" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a function to prepare the dataset</h3>
<div class="callout-content">
<p>Hint 1: Your function should accept the training images and labels as
arguments</p>
<p>Hint 2: Use 20% split for validation and a random state of ‘42’</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># create a function to prepare the training dataset</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="kw">def</span> prepare_dataset(_____, _____):</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    </span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    <span class="co"># normalise the RGB values to be between 0 and 1</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    _____</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    </span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    <span class="co"># one hot encode the training labels</span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>    _____</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>    </span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>    <span class="co"># split the training data into training and validation set</span></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>    _____</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>    <span class="cf">return</span> _____</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="kw">def</span> prepare_dataset(train_images, train_labels):</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    </span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="co"># normalise the RGB values to be between 0 and 1</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    train_images <span class="op">=</span> train_images <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    </span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    <span class="co"># one hot encode the training labels</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>    train_labels <span class="op">=</span> keras.utils.to_categorical(train_labels, <span class="bu">len</span>(class_names))</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    </span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>    <span class="co"># split the training data into training and validation set</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>    train_images, val_images, train_labels, val_labels <span class="op">=</span> train_test_split(</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>    train_images, train_labels, test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>    <span class="cf">return</span> train_images, val_images, train_labels, val_labels</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Inspect the labels before and after data preparation to visualise
one-hot encoding.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'train_labels before one hot encoding'</span>)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(train_labels)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="co"># prepare the dataset for training</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>train_images, val_images, train_labels, val_labels <span class="op">=</span> prepare_dataset(train_images, train_labels)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'train_labels after one hot encoding'</span>)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="bu">print</span>(train_labels)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>train_labels before one hot encoding
[[6]
 [9]
 [9]
 ...
 [9]
 [1]
 [1]]

train_labels after one hot encoding
[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 0. 1.]
 ...
 [0. 0. 0. ... 0. 0. 1.]
 [0. 1. 0. ... 0. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]]</code></pre>
</div>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>WAIT I thought there were TEN classes!? Where is the rest of the
data?</p>
<p>The Spyder IDE uses the ‘…’ notation when it “hides” some of the data
for display purposes.</p>
<p>To view the entire array, go the Variable Explorer in the upper right
hand corner of your Spyder IDE and double click on the ‘train_labels’
object. This will open a new window that shows all of the columns.</p>
<figure><img src="fig/02_spyder_onehot_train_labels_inFULL.png" alt="Screenshot of Spyder window displaying the entire train_labels array." class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div id="challenge-training-and-validation" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-training-and-validation" class="callout-inner">
<h3 class="callout-title">CHALLENGE Training and Validation</h3>
<div class="callout-content">
<p>Inspect the training and validation sets we created.</p>
<p>How many samples does each set have and are the classes well
balanced?</p>
<p>Hint: Use <code>np.sum()</code> on the ’*_labels’ to find out if the
classes are well balanced.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>A. Training Set</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of training set images'</span>, train_images.shape[<span class="dv">0</span>])</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of images in each class:</span><span class="ch">\n</span><span class="st">'</span>, train_labels.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of training set images: 40000
Number of images in each class:
 [4027. 4021. 3970. 3977. 4067. 3985. 4004. 4006. 3983. 3960.]</code></pre>
</div>
<p>B. Validation Set (we can use the same code as the training set)</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of validation set images'</span>, val_images.shape[<span class="dv">0</span>])</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Nmber of images in each class:</span><span class="ch">\n</span><span class="st">'</span>, val_labels.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of validation set images: 10000
Nmber of images in each class:
 [ 973.  979. 1030. 1023.  933. 1015.  996.  994. 1017. 1040.]</code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler3" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler3" aria-expanded="false" aria-controls="collapseSpoiler3">
  <h3 class="accordion-header" id="headingSpoiler3">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div> WANT TO KNOW MORE: Data Splitting Techniques </h3>
</button>
<div id="collapseSpoiler3" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler3" aria-labelledby="headingSpoiler3">
<div class="accordion-body">
<p>ChatGPT</p>
<p>Data is typically split into the training, validation, and test data
sets using a process called data splitting or data partitioning. There
are various methods to perform this split, and the choice of technique
depends on the specific problem, dataset size, and the nature of the
data. Here are some common approaches:</p>
<p><strong>Hold-Out Method:</strong></p>
<ul>
<li><p>In the hold-out method, the dataset is divided into two parts
initially: a training set and a test set.</p></li>
<li><p>The training set is used to train the model, and the test set is
kept completely separate to evaluate the model’s final
performance.</p></li>
<li><p>This method is straightforward and widely used when the dataset
is sufficiently large.</p></li>
</ul>
<p><strong>Train-Validation-Test Split:</strong></p>
<ul>
<li><p>The dataset is split into three parts: the training set, the
validation set, and the test set.</p></li>
<li><p>The training set is used to train the model, the validation set
is used to tune hyperparameters and prevent overfitting during training,
and the test set is used to assess the final model performance.</p></li>
<li><p>This method is commonly used when fine-tuning model
hyperparameters is necessary.</p></li>
</ul>
<p><strong>K-Fold Cross-Validation:</strong></p>
<ul>
<li><p>In k-fold cross-validation, the dataset is divided into k subsets
(folds) of roughly equal size.</p></li>
<li><p>The model is trained and evaluated k times, each time using a
different fold as the test set while the remaining k-1 folds are used as
the training set.</p></li>
<li><p>The final performance metric is calculated as the average of the
k evaluation results, providing a more robust estimate of model
performance.</p></li>
<li><p>This method is particularly useful when the dataset size is
limited, and it helps in better utilizing available data.</p></li>
</ul>
<p><strong>Stratified Sampling:</strong></p>
<ul>
<li><p>Stratified sampling is used when the dataset is imbalanced,
meaning some classes or categories are underrepresented.</p></li>
<li><p>The data is split in such a way that each subset (training,
validation, or test) maintains the same class distribution as the
original dataset.</p></li>
<li><p>This ensures all classes are well-represented in each subset,
which is important to avoid biased model evaluation.</p></li>
</ul>
<p>It’s important to note that the exact split ratios (e.g., 80-10-10 or
70-15-15) may vary depending on the problem, dataset size, and specific
requirements. Additionally, data splitting should be performed randomly
to avoid introducing any biases into the model training and evaluation
process.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="data-preprocessing-completed">Data preprocessing completed!<a class="anchor" aria-label="anchor" href="#data-preprocessing-completed"></a>
</h2>
<hr class="half-width">
<p>We now have a function we can use throughout the lesson to preprocess
our data which means we are ready to learn how to build a CNN like we
used in the introduction.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Image datasets can be found online or created uniquely for your
research question.</li>
<li>Images consist of pixels arranged in a particular order.</li>
<li>Image data is usually preprocessed before use in a CNN for
efficiency, consistency, and robustness.</li>
<li>Input data generally consists of three sets: a training set used to
fit model parameters; a validation set used to evaluate the model fit on
training data; a test set used to evaluate the final model
performance.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-03-build-cnn"><p>Content from <a href="03-build-cnn.html">Build a Convolutional Neural Network</a></p>
<hr>
<p>Last updated on 2025-06-11 |

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/episodes/03-build-cnn.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a (artificial) neural network (ANN)?</li>
<li>How is a convolutional neural network (CNN) different from an
ANN?</li>
<li>What are the types of layers used to build a CNN?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how a convolutional neural network (CNN) differs from an
artificial neural network (ANN).</li>
<li>Explain the terms: kernel, filter.</li>
<li>Know the different layers: convolutional, pooling, flatten,
dense.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="neural-networks">Neural Networks<a class="anchor" aria-label="anchor" href="#neural-networks"></a>
</h2>
<hr class="half-width">
<p>A <strong>neural network</strong> is an artificial intelligence
technique loosely based on the way neurons in the brain work.</p>
<div class="section level3">
<h3 id="a-single-neuron">A single neuron<a class="anchor" aria-label="anchor" href="#a-single-neuron"></a>
</h3>
<p>Each <strong>neuron</strong> will:</p>
<ul>
<li>Take one or more inputs (<span class="math inline">\(x_1, x_2,
...\)</span>), e.g., floating point numbers, each with a corresponding
weight.</li>
<li>Calculate the weighted sum of the inputs where ($w_1, w_2, … $)
indicate weights.</li>
<li>Add an extra constant weight (i.e. a bias term) to this weighted
sum.</li>
<li>Apply a non-linear function to the bias-adjusted weighted sum.</li>
<li>Return one output value, again a floating point number.</li>
</ul>
<p>One example equation to calculate the output for a neuron is:</p>
<p><span class="math inline">\(output=ReLU(∑i(xi∗wi)+bias)\)</span></p>
<figure><img src="fig/03_neuron.png" alt="diagram of a single neuron taking multiple inputs and their associated weights in and then applying an activation function to predict a single output" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="combining-multiple-neurons-into-a-network">Combining multiple neurons into a network<a class="anchor" aria-label="anchor" href="#combining-multiple-neurons-into-a-network"></a>
</h3>
<p>Multiple neurons can be joined together by connecting the output of
one to the input of another. These connections are also associated with
weights that determine the ‘strength’ of the connection, and these
weights are also adjusted during training. In this way, the combination
of neurons and connections describe a computational graph, an example
can be seen in the image below.</p>
<p>In most neural networks neurons are aggregated into layers. Signals
travel from the input layer to the output layer, possibly through one or
more intermediate layers called hidden layers. The image below
illustrates an example of a neural network with three layers, each
circle is a neuron, each line is an edge and the arrows indicate the
direction data moves in.</p>
<figure><img src="fig/03_neural_net.png" alt="diagram of a neural with four neurons taking multiple inputs and their weights and predicting multiple outputs" class="figure mx-auto d-block"><div class="figcaption">The image above is by Glosser.ca, <a href="https://creativecommons.org/licenses/by-sa/3.0" class="external-link">CC BY-SA 3.0</a>,
via Wikimedia Commons, <a href="https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg" class="external-link">original
source</a>
</div>
</figure><p>Neural networks aren’t a new technique, they have been around since
the late 1940s. But until around 2010 neural networks tended to be quite
small, consisting of only 10s or perhaps 100s of neurons. This limited
them to only solving quite basic problems. Around 2010 improvements in
computing power and the algorithms for training the networks made much
larger and more powerful networks practical. These are known as deep
neural networks or Deep Learning.</p>
</div>
</section><section><h2 class="section-heading" id="convolutional-neural-networks">Convolutional Neural Networks<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks"></a>
</h2>
<hr class="half-width">
<p>A convolutional neural network (CNN) is a type of artificial neural
network (ANN) most commonly applied to analyze visual imagery. They are
specifically designed for processing grid-like data, such as images, by
leveraging convolutional layers that preserve spatial relationships,
when extracting features.</p>
<div class="section level3">
<h3 id="step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model">Step 4. Build an architecture from scratch or choose a pretrained
model<a class="anchor" aria-label="anchor" href="#step-4--build-an-architecture-from-scratch-or-choose-a-pretrained-model"></a>
</h3>
<p>Let us explore how to build a neural network from scratch. Although
this sounds like a daunting task, with Keras it is surprisingly
straightforward. With Keras you compose a neural network by creating
layers and linking them together.</p>
</div>
<div class="section level3">
<h3 id="parts-of-a-neural-network">Parts of a neural network<a class="anchor" aria-label="anchor" href="#parts-of-a-neural-network"></a>
</h3>
<p>There are three main components of a neural network:</p>
<ul>
<li>CNN Part 1. Input Layer</li>
<li>CNN Part 2. Hidden Layers</li>
<li>CNN Part 3. Output Layer</li>
</ul>
<p>The output from each layer becomes the input to the next layer.</p>
<div class="section level4">
<h4 id="cnn-part-1--input-layer">CNN Part 1. Input Layer<a class="anchor" aria-label="anchor" href="#cnn-part-1--input-layer"></a>
</h4>
<p>The Input in Keras gets special treatment when images are used. Keras
automatically calculates the number of inputs and outputs a specific
layer needs and therefore how many edges need to be created. This means
we just need to let Keras know how big our input is going to be. We do
this by instantiating a <code>keras.Input</code> class and passing it a
tuple to indicate the dimensionality of the input data. In Python, a
<strong>tuple</strong> is a data type used to store collections of data.
It is similar to a list, but tuples are immutable, meaning once they are
created, their contents cannot be changed.</p>
<p>The input layer is created with the <code>keras.Input</code> function
and its first parameter is the expected shape of the input:</p>
<pre><code><span><span class="fu">keras.Input</span><span class="op">(</span>shape<span class="op">=</span><span class="va">None</span>, batch_size<span class="op">=</span><span class="va">None</span>, dtype<span class="op">=</span><span class="va">None</span>, sparse<span class="op">=</span><span class="va">None</span>, batch_shape<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="va">None</span>, tensor<span class="op">=</span><span class="va">None</span><span class="op">)</span></span></code></pre>
<p>In our case, the shape of an image is defined by its pixel dimensions
and number of channels:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># recall the shape of the images in our dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="bu">print</span>(train_images.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(40000, 32, 32, 3) # number of images, image width in pixels, image height in pixels, number of channels (RGB)</code></pre>
</div>
<div id="challenge-create-the-input-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-the-input-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create the input layer for our network</h3>
<div class="callout-content">
<p>Hint 1: Specify shape argument only and use defaults for the
rest.</p>
<p>Hint 2: The shape of our input dataset includes the total number of
images. We want to take a slice of the shape for a single individual
image to use an input.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># CNN Part 1</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co"># Input layer of 32x32 images with three channels (RGB)</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>inputs_intro <span class="op">=</span> keras.Input(_____)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># CNN Part 1
# Input layer of 32x32 images with three channels (RGB)
inputs_intro = keras.Input(shape=train_images.shape[1:])</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="cnn-part-2--hidden-layers">CNN Part 2. Hidden Layers<a class="anchor" aria-label="anchor" href="#cnn-part-2--hidden-layers"></a>
</h4>
<p>The next component consists of the so-called hidden layers of the
network.</p>
<p>In a neural network, the input layer receives the raw data, and the
output layer produces the final predictions or classifications. These
layers’ contents are directly observable because you can see the input
data and the network’s output predictions.</p>
<p>However, the hidden layers, which lie between the input and output
layers, contain intermediate representations of the input data. These
representations are transformed through various mathematical operations
performed by the network’s neurons. The specific values and operations
within these hidden layers are not directly accessible or interpretable
from the input or output data. Therefore, they are considered “hidden”
from external observation or inspection.</p>
<p>In a CNN, the hidden layers typically consist of convolutional,
pooling, reshaping (e.g., Flatten), and dense layers.</p>
<p>Check out the <a href="https://keras.io/api/layers/" class="external-link">Layers API</a>
section of the Keras documentation for each layer type and its
parameters.</p>
<div class="section level5">
<h5 id="convolutional-layers">
<strong>Convolutional Layers</strong><a class="anchor" aria-label="anchor" href="#convolutional-layers"></a>
</h5>
<p>A <strong>convolutional</strong> layer is a fundamental building
block in a CNN designed for processing structured, gridded data, such as
images. It applies convolution operations to input data using learnable
filters or kernels, extracting local patterns and features (e.g. edges,
corners). These filters enable the network to capture hierarchical
representations of visual information, allowing for effective feature
learning.</p>
<p>To find the particular features of an image, CNNs make use of a
concept from image processing that precedes Deep Learning.</p>
<p>A <strong>convolution matrix</strong>, or <strong>kernel</strong>, is
a matrix transformation that we ‘slide’ over the image to calculate
features at each position of the image. For each pixel, we calculate the
matrix product between the kernel and the pixel with its surroundings.
Here is one example of a 3x3 kernel used to detect edges:</p>
<pre><code>[[-1, -1, -1],
 [0,   0,  0]
 [1,   1,  1]]</code></pre>
<p>This kernel will give a high value to a pixel if it is on a
horizontal border between dark and light areas.</p>
<p>In the following image, the effect of such a kernel on the values of
a single-channel image stands out. The red cell in the output matrix is
the result of multiplying and summing the values of the red square in
the input, and the kernel. Applying this kernel to a real image
demonstrates it does indeed detect horizontal edges.</p>
<figure><img src="fig/03_conv_matrix.png" alt="6x5 input matrix representing a single colour channel image being multipled by a 3x3 kernel to produce a 4x4 output matrix to detect horizonal edges in an image " class="figure mx-auto d-block"></figure><figure><img src="fig/03_conv_image.png" alt="single colour channel image of a cat multiplied by a 3x3 kernel to produce an image of a cat where the edges  stand out" class="figure mx-auto d-block"></figure><p>There are several types of convolutional layers available in Keras
depending on your application. We use the two-dimensional layer
typically used for images:</p>
<pre><code>keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding="valid", activation=None, **kwargs)</code></pre>
<ul>
<li>
<code>filters</code> is the number of filters in this layer.
<ul>
<li>This is one of the hyperparameters of our system and should be
chosen carefully.</li>
<li>Good practice is to start with a relatively small number of filters
in the first layer to prevent overfitting.</li>
<li>Choosing a number of filters as a power of two (e.g., 32, 64, 128)
is common.</li>
</ul>
</li>
<li>
<code>kernel size</code> is the size of the convolution matrix which
we already discussed. - Smaller kernels are often used to capture
fine-grained features and odd-sized filters are preferred because they
have a centre pixel which helps maintain spatial symmetry during
convolutions.</li>
<li>
<code>activation</code> specifies which activation function to
use.</li>
</ul>
<p>When specifying layers, remember each layer’s output is the input to
the next layer. We must create a variable to store a reference to the
output so we can pass it to the next layer. The basic format for doing
this is:</p>
<p>output_variable = layer_name(layer_arguments)(input_variable)</p>
<div id="challenge-create-a-2d-convolutional-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-2d-convolutional-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a 2D convolutional layer for our network</h3>
<div class="callout-content">
<p>Create a Conv2D layer with 16 filters, a 3x3 kernel size, and the
‘relu’ activation function.</p>
<p>Here we choose <strong>relu</strong> which is one of the most
commonly used in deep neural networks that is proven to work well. We
will discuss activation functions later in <strong>Step 9. Tune
hyperparameters</strong> but to satisfy your curiosity,
<code>ReLU</code> stands for Rectified Linear Unit (ReLU).</p>
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># CNN Part 2</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="co"># Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Conv2D(filters<span class="op">=</span>_____, kernel_size<span class="op">=</span>_____, activation<span class="op">=</span>_____)(_____)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># CNN Part 2
# Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation
x_intro = keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(inputs_intro)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="playing-with-convolutions" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="playing-with-convolutions" class="callout-inner">
<h3 class="callout-title">Playing with convolutions</h3>
<div class="callout-content">
<p>Convolutions applied to images can be hard to grasp at first.
Fortunately, there are resources out there that enable users to
interactively play around with images and convolutions:</p>
<ul>
<li><p><a href="https://setosa.io/ev/image-kernels/" class="external-link">Image kernels
explained</a> illustrates how different convolutions can achieve certain
effects on an image, like sharpening and blurring.</p></li>
<li><p>The <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" class="external-link">convolutional
neural network cheat sheet</a> provides animated examples of the
different components of convolutional neural nets.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="pooling-layers">
<strong>Pooling Layers</strong><a class="anchor" aria-label="anchor" href="#pooling-layers"></a>
</h5>
<p>The convolutional layers are often intertwined with
<strong>Pooling</strong> layers. As opposed to the convolutional layer
used in feature extraction, the pooling layer alters the dimensions of
the image and reduces it by a scaling factor effectively decreasing the
resolution of your picture.</p>
<p>The rationale behind this is that higher layers of the network should
focus on higher-level features of the image. By introducing a pooling
layer, the subsequent convolutional layer has a broader ‘view’ on the
original image.</p>
<p>Similar to convolutional layers, Keras offers several pooling layers
and one used for images (2D spatial data):</p>
<pre><code>keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None, name=None, **kwargs)</code></pre>
<ul>
<li>
<code>pool_size</code>, i.e., the size of the pooling window
<ul>
<li>In Keras, the default is usually (2, 2)</li>
</ul>
</li>
</ul>
<p>The function downsamples the input along its spatial dimensions
(height and width) by taking the <strong>maximum</strong> value over an
input window (of size defined by pool_size) for each channel of the
input. By taking the maximum instead of the average, the most prominent
features in the window are emphasized.</p>
<p>For example, a 2x2 pooling size reduces the width and height of the
input by a factor of 2. Empirically, a 2x2 pooling size has been found
to work well in various for image classification tasks and also strikes
a balance between down-sampling for computational efficiency and
retaining important spatial information.</p>
<div id="challenge-create-a-pooling-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-pooling-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a Pooling layer for our network</h3>
<div class="callout-content">
<p>Create a pooling layer with input window sized 2x2.</p>
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># Pooling layer with input window sized 2x2</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.MaxPooling2D(pool_size<span class="op">=</span>_____)(_____)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># Pooling layer with input window sized 2x2
x_intro = keras.layers.MaxPooling2D(pool_size=(2,2))(x_intro)</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="dense-layers">
<strong>Dense layers</strong><a class="anchor" aria-label="anchor" href="#dense-layers"></a>
</h5>
<p>A <strong>dense</strong> layer is a fully connected layer where each
neuron receives input from every neuron in the previous layer. When
connecting the layer to its input and output layers every neuron in the
dense layer gets an edge (i.e. connection) to <strong>all</strong> of
the input neurons and <strong>all</strong> of the output neurons.</p>
<figure><img src="fig/03-neural_network_sketch_dense.png" alt="diagram of a neural network with multiple inputs feeding into to two seperate dense layers with connections between all the inputs and outputs" class="figure mx-auto d-block"></figure><p>This layer aggregates global information about the features learned
in previous layers to make a decision about the class of the input.</p>
<p>In Keras, a densely-connected layer is defined:</p>
<pre><code>keras.layers.Dense(units, activation=None, **kwargs)</code></pre>
<ul>
<li>`units in this case refers to the number of neurons.</li>
</ul>
<p>The choice of how many neurons to specify is often determined through
experimentation and can impact the performance of our CNN. Too few
neurons may not capture complex patterns in the data but too many
neurons may lead to overfitting.</p>
<div id="challenge-create-a-dense-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-dense-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a Dense layer for our network</h3>
<div class="callout-content">
<p>Create a dense layer with 64 neurons and ‘relu’ activation.</p>
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># Dense layer with 64 neurons and ReLU activation</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>_____, activation<span class="op">=</span>_____)(_____)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="co"># Dense layer with 64 neurons and ReLU activation</span></span>
<span><span class="va">x_intro</span> <span class="op">=</span> <span class="fu">keras.layers.Dense</span><span class="op">(</span>units<span class="op">=</span><span class="fl">64</span>, activation<span class="op">=</span><span class="st">'relu'</span><span class="op">)</span><span class="op">(</span><span class="va">x_intro</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level5">
<h5 id="reshaping-layers-flatten">
<strong>Reshaping Layers: Flatten</strong><a class="anchor" aria-label="anchor" href="#reshaping-layers-flatten"></a>
</h5>
<p>The next type of hidden layer used in our introductory model is a
type of reshaping layer defined in Keras by the
<code>keras.layers.Flatten</code> class. It is necessary when
transitioning from convolutional and pooling layers to fully connected
layers.</p>
<pre><code>keras.layers.Flatten(data_format=None, **kwargs)</code></pre>
<p>The <strong>Flatten</strong> layer converts the output of the
previous layer into a single one-dimensional vector that can be used as
input for a dense layer.</p>
<div id="challenge-create-a-flatten-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-flatten-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a Flatten layer for our network</h3>
<div class="callout-content">
<p>Create a flatten layer.</p>
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>x_intro <span class="op">=</span> keras.layers.Flatten()(_____)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="co"># Flatten layer to convert 2D feature maps into a 1D vector</span></span>
<span><span class="va">x_intro</span> <span class="op">=</span> <span class="fu">keras.layers.Flatten</span><span class="op">(</span><span class="op">)</span><span class="op">(</span><span class="va">x_intro</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div> What does Flatten mean exactly? </h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p>A flatten layer function is typically used to transform the
two-dimensional arrays (matrices) generated by the convolutional and
pooling layers into a one-dimensional array. This is necessary when
transitioning from the convolutional/pooling layers to the fully
connected layers, which require one-dimensional input.</p>
<p>During the convolutional and pooling operations, a neural network
extracts features from the input images, resulting in multiple feature
maps, each represented by a matrix. These feature maps capture different
aspects of the input image, such as edges, textures, or patterns.
However, to feed these features into a fully connected layer for
classification or regression tasks, they must be a single vector.</p>
<p>The flatten layer takes each element from the feature maps and
arranges them into a single long vector, concatenating them along a
single dimension. This transformation preserves the spatial
relationships between the features in the original image while providing
a suitable format for the fully connected layers to process.</p>
</div>
</div>
</div>
</div>
<div id="is-one-layer-of-each-type-enough" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="is-one-layer-of-each-type-enough" class="callout-inner">
<h3 class="callout-title">Is one layer of each type enough?</h3>
<div class="callout-content">
<p>Not for complex data!</p>
<p>A typical architecture for image classification is likely to include
at least one convolutional layer, one pooling layer, one or more dense
layers, and possibly a flatten layer.</p>
<p>Convolutional and Pooling layers are often used together in multiple
sets to capture a wider range of features and learn more complex
representations of the input data. Using this technique, the network can
learn a hierarchical representation of features, where simple features
detected in early layers are combined to form more complex features in
deeper layers.</p>
<p>There isn’t a strict rule of thumb for the number of sets of
convolutional and pooling layers to start with, however, there are some
guidelines.</p>
<p>We are starting with a relatively small and simple architecture
because we are limited in time and computational resources. A simple CNN
with one or two sets of convolutional and pooling layers can still
achieve decent results for many tasks but for your network you will
experiment with different architectures.</p>
</div>
</div>
</div>
<div id="challenge-using-the-four-layer-types-above-create-a-hidden-layer-architecture" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-using-the-four-layer-types-above-create-a-hidden-layer-architecture" class="callout-inner">
<h3 class="callout-title">CHALLENGE Using the four layer types above, create a hidden layer architecture</h3>
<div class="callout-content">
<p>Create a hidden layer architecture with the following
specifications:</p>
<ul>
<li>2 sets of Conv2D and Pooling layers, with 16 and 32 filters
respectively</li>
<li>1 Flatten layer</li>
<li>1 Dense layer with 64 neurons and ‘relu’ activation</li>
</ul>
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
</div>
</div>
</div>
<div id="accordionSolution6" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution6" aria-expanded="false" aria-controls="collapseSolution6">
  <h4 class="accordion-header" id="headingSolution6"> Show me the solution </h4>
</button>
<div id="collapseSolution6" class="accordion-collapse collapse" aria-labelledby="headingSolution6" data-bs-parent="#accordionSolution6">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># CNN Part 2
# Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation
x_intro = keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(inputs_intro)
# Pooling layer with input window sized 2x2
x_intro = keras.layers.MaxPooling2D(pool_size=(2,2))(x_intro)
# Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation
x_intro = keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x_intro)
# Second Pooling layer with input window sized 2x2
x_intro = keras.layers.MaxPooling2D(pool_size=(2,2))(x_intro)
# Flatten layer to convert 2D feature maps into a 1D vector
x_intro = keras.layers.Flatten()(x_intro)
# Dense layer with 64 neurons and ReLU activation
x_intro = keras.layers.Dense(units=64, activation='relu')(x_intro)</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level4">
<h4 id="cnn-part-3--output-layer">CNN Part 3. Output Layer<a class="anchor" aria-label="anchor" href="#cnn-part-3--output-layer"></a>
</h4>
<p>Recall for the outputs we asked ourselves what we want to identify
from the data. If we are performing a classification problem, then
typically we have one output for each potential class.</p>
<p>In traditional CNN architectures, a dense layer is typically used as
the final layer for classification. This dense layer receives the
flattened feature maps from the preceding convolutional and pooling
layers and outputs the final class probabilities or regression
values.</p>
<p>For multiclass data, the <code>softmax</code> activation is used
instead of <code>relu</code> because it helps the computer give each
option (class) a likelihood score, and the scores add up to 100 per
cent. This way, it’s easier to pick the one the computer thinks is most
probable.</p>
<div id="challenge-create-an-output-layer-for-our-network" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-an-output-layer-for-our-network" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create an Output layer for our network</h3>
<div class="callout-content">
<p>Use a dense layer to create the output layer for a classification
problem with 10 possible classes.</p>
<p>Hint 1: The input to each layer is the output of the previous
layer.</p>
<p>Hint 2: The units (neurons) should be the same as number of classes
as our dataset.</p>
<p>Hint 3: Use softmax activation.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># CNN Part 3</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>outputs_intro <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>_____, activation<span class="op">=</span>_____)(_____)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution7" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution7" aria-expanded="false" aria-controls="collapseSolution7">
  <h4 class="accordion-header" id="headingSolution7"> Show me the solution </h4>
</button>
<div id="collapseSolution7" class="accordion-collapse collapse" aria-labelledby="headingSolution7" data-bs-parent="#accordionSolution7">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="co"># CNN Part 3</span></span>
<span><span class="co"># Output layer with 10 units (one for each class) and softmax activation</span></span>
<span><span class="va">outputs_intro</span> <span class="op">=</span> <span class="fu">keras.layers.Dense</span><span class="op">(</span>units<span class="op">=</span><span class="fl">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span><span class="op">)</span><span class="op">(</span><span class="va">x_intro</span><span class="op">)</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="putting-it-all-together">Putting it all together<a class="anchor" aria-label="anchor" href="#putting-it-all-together"></a>
</h2>
<hr class="half-width">
<p>Once you decide on the initial architecture of your CNN, the last
step to create the model is to bring all of the parts together using the
<code>keras.Model</code> class.</p>
<p>There are several ways of grouping the layers into an object as
described in the <a href="https://keras.io/api/models/" class="external-link">Keras Models
API</a>.</p>
<p>We will use the Functional API to create our model using the inputs
and outputs defined in this episode.</p>
<pre><code><span><span class="fu">keras.Model</span><span class="op">(</span>inputs<span class="op">=</span><span class="va">inputs</span>, outputs<span class="op">=</span><span class="va">outputs</span><span class="op">)</span></span></code></pre>
<p>Note that there is additional argument that can be passed to the
keras.Model class called ‘name’ that takes a string. Although it is no
longer specified in the documentation, the ‘name’ argument is useful
when deciding among different architectures.</p>
<div id="challenge-create-a-function-that-defines-an-introductory-cnn" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-create-a-function-that-defines-an-introductory-cnn" class="callout-inner">
<h3 class="callout-title">CHALLENGE Create a function that defines an introductory CNN</h3>
<div class="callout-content">
<p>Using the keras.Model class and the input, hidden, and output layers
from the previous challenges, create a function that returns the CNN
from the introduction.</p>
<p>Hint 1: Name the model “cifar_model_intro”</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="kw">def</span> create_model_intro():</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>    <span class="co"># CNN Part 1</span></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>    _____</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>    </span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>    <span class="co"># CNN Part 2</span></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>    _____</span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>    </span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a>    <span class="co"># CNN Part 3</span></span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a>    _____</span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a>    </span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a>    <span class="co"># create the model</span></span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a>    model_intro <span class="op">=</span> keras.Model(inputs <span class="op">=</span> _____, </span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a>                              outputs <span class="op">=</span> _____, </span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a>                              name <span class="op">=</span> _____)</span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a>    </span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a>    <span class="cf">return</span> model_intro</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution8" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution8" aria-expanded="false" aria-controls="collapseSolution8">
  <h4 class="accordion-header" id="headingSolution8"> Show me the solution </h4>
</button>
<div id="collapseSolution8" class="accordion-collapse collapse" aria-labelledby="headingSolution8" data-bs-parent="#accordionSolution8">
<div class="accordion-body">
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>def create_model_intro():

    # CNN Part 1
    # Input layer of 32x32 images with three channels (RGB)
    inputs_intro = keras.Input(shape=train_images.shape[1:])

    # CNN Part 2
    # Convolutional layer with 16 filters, 3x3 kernel size, and ReLU activation
    x_intro = keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu')(inputs_intro)
    # Pooling layer with input window sized 2x2
    x_intro = keras.layers.MaxPooling2D(pool_size=(2,2))(x_intro)
    # Second Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation
    x_intro = keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x_intro)
    # Second Pooling layer with input window sized 2x2
    x_intro = keras.layers.MaxPooling2D(pool_size=(2,2))(x_intro)
    # Flatten layer to convert 2D feature maps into a 1D vector
    x_intro = keras.layers.Flatten()(x_intro)
    # Dense layer with 64 neurons and ReLU activation
    x_intro = keras.layers.Dense(units=64, activation='relu')(x_intro)

    # CNN Part 3
    # Output layer with 10 units (one for each class) and softmax activation
    outputs_intro = keras.layers.Dense(units=10, activation='softmax')(x_intro)

    # create the model
    model_intro = keras.Model(inputs = inputs_intro,
                              outputs = outputs_intro,
                              name = "cifar_model_intro")

    return model_intro</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>We now have a function that defines the introduction model.</p>
<p>We can use this function to create the introduction model and and
view a summary of its structure using the <code>Model.summary</code>
method.</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="co"># create the introduction model</span></span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>model_intro <span class="op">=</span> create_model_intro()</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>model_intro.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "cifar_model_intro"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_1 (InputLayer)        [(None, 32, 32, 3)]       0

 conv2d (Conv2D)             (None, 30, 30, 16)        448

 max_pooling2d (MaxPooling2  (None, 15, 15, 16)        0
 D)

 conv2d_1 (Conv2D)           (None, 13, 13, 32)        4640

 max_pooling2d_1 (MaxPoolin  (None, 6, 6, 32)          0
 g2D)

 flatten (Flatten)           (None, 1152)              0

 dense (Dense)               (None, 64)                73792

 dense_1 (Dense)             (None, 10)                650

=================================================================
Total params: 79530 (310.66 KB)
Trainable params: 79530 (310.66 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div> Explain the summary output </h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" aria-labelledby="headingSpoiler2" data-bs-parent="#accordionSpoiler2">
<div class="accordion-body">
<p>The Model.summary() output has three columns:</p>
<ol style="list-style-type: decimal">
<li>Layer (type) lists the name and type of each layer.  
<ul>
<li>Remember the ‘name’ argument we used to name our model? This
argument can also be supplied to each layer. If a name is not provided,
Keras will assign a unique identifier starting from 1 and incrementing
for each new layer of the same type within the model.  </li>
</ul>
</li>
<li>Output Shape describes the shape of the output produced by each
layer as (batch_size, height, width, channels).  
<ul>
<li>Batch size is the number of samples processed in each batch during
training or inference. This dimension is often denoted as None in the
summary because the batch size can vary and is typically specified
during model training or inference.</li>
<li>Height, Width, Channels: The remaining dimensions represent the
spatial dimensions and the number of channels in the output tensor. For
convolutional layers and pooling layers, the height and width dimensions
typically decrease as the network progresses through the layers due to
the spatial reduction caused by convolution and pooling operations. The
number of channels may change depending on the number of filters in the
convolutional layer or the design of the network architecture.</li>
<li>For example, in a convolutional layer, the output shape (None, 30,
30, 16) means:
<ul>
<li>None: The batch size can vary.</li>
<li>30: The height and width of the output feature maps are both 30
pixels.</li>
<li>16: There are 16 channels in the output feature maps, indicating
that the layer has 16 filters.  </li>
</ul>
</li>
</ul>
</li>
<li>Param # displays the number of parameters (weights and biases)
associated with each layer.  
<ul>
<li>The total number of parameters in a layer is calculated as follows:
<ul>
<li>Total parameters = (number of input units) × (number of output
units) + (number of output units)</li>
</ul>
</li>
<li>At the bottom of the Model.summary() you will find the number of
Total parameters and their size; the number of Trainable parameters and
their size; and the number of Non-trainable parameters and their size.
<ul>
<li>In most cases, the total number of parameters will match the number
of trainable parameters. In other cases, such as models using
normalization layers, there will be some parameters that are fixed
during training and not trainable.</li>
</ul>
</li>
<li>Disclaimer: We explicitly decided to focus on building a
foundational understanding of convolutional neural networks for this
course without delving into the detailed calculation of parameters.
However, as your progress on your deep learning journey it will become
increasingly important for you to understand parameter calculation in
order to optimize model performance, troubleshoot issues, and design
more efficient CNN architectures.</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
</div>
<div id="how-to-choose-an-architecture" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="how-to-choose-an-architecture" class="callout-inner">
<h3 class="callout-title">How to choose an architecture?</h3>
<div class="callout-content">
<p>For this neural network, we had to make many choices, including the
number of hidden neurons. Other choices to be made are the number of
layers and type of layers. You might wonder how you should make these
architectural choices. Unfortunately, there are no clear rules to follow
here, and it often boils down to a lot of trial and error. It is
recommended to explore what others have done with similar data sets and
problems. Another best practice is to start with a relatively simple
architecture and then add layers and tweak the network to test if
performance increases.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="we-have-a-model-now-what">We have a model now what?<a class="anchor" aria-label="anchor" href="#we-have-a-model-now-what"></a>
</h2>
<hr class="half-width">
<p>This CNN should be able to run with the CIFAR-10 data set and provide
reasonable results for basic classification tasks. However, do keep in
mind this model is relatively simple, and its performance may not be as
high as more complex architectures. The reason it’s called deep learning
is because, in most cases, the more layers we have, i.e. the deeper and
more sophisticated CNN architecture we use, the better the
performance.</p>
<p>How can we judge a model’s performance? We can inspect a couple
metrics produced during the training process to detect whether our model
is underfitting or overfitting. To do that, we continue with the next
steps in our Deep Learning workflow, <strong>Step 5. Choose a loss
function and optimizer</strong> and <strong>Step 6. Train
model</strong>.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Artificial neural networks (ANN) are a machine learning technique
based on a model inspired by groups of neurons in the brain.</li>
<li>Convolution neural networks (CNN) are a type of ANN designed for
image classification and object detection.</li>
<li>The number of filters corresponds to the number of distinct features
the layer is learning to recognise whereas the kernel size determines
the level of features being captured.</li>
<li>A CNN can consist of many types of layers including convolutional,
pooling, flatten, and dense (fully connected) layers</li>
<li>Convolutional layers are responsible for learning features from the
input data.</li>
<li>Pooling layers are often used to reduce the spatial dimensions of
the data.</li>
<li>The flatten layer is used to convert the multi-dimensional output of
the convolutional and pooling layers into a flat vector.</li>
<li>Dense layers are responsible for combining features learned by the
previous layers to perform the final classification.</li>
</ul>
</div>
</div>
</div>
<!-- Collect your link references at the bottom of your document -->
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries/workbench-template-md/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries/workbench-template-md/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries/workbench-template-md/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries/workbench-template-md/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:training@qcif.edu.au">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.12" class="external-link">sandpaper (0.16.12)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.6" class="external-link">varnish (1.0.6)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries.github.io/workbench-template-md/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "machine learning, classification, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries.github.io/workbench-template-md/aio.html",
  "identifier": "https://carpentries.github.io/workbench-template-md/aio.html",
  "dateCreated": "2025-06-01",
  "dateModified": "2025-06-11",
  "datePublished": "2025-06-11"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

